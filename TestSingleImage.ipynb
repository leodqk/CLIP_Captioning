{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":2598787,"sourceType":"datasetVersion","datasetId":1573501}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/rmokady/CLIP_prefix_caption.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-30T03:21:55.326882Z","iopub.execute_input":"2024-07-30T03:21:55.327250Z","iopub.status.idle":"2024-07-30T03:21:56.989953Z","shell.execute_reply.started":"2024-07-30T03:21:55.327211Z","shell.execute_reply":"2024-07-30T03:21:56.989089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda env create -f /kaggle/working/CLIP_prefix_caption/environment.yml","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:22:48.904470Z","iopub.execute_input":"2024-07-30T03:22:48.905400Z","iopub.status.idle":"2024-07-30T03:28:40.146501Z","shell.execute_reply.started":"2024-07-30T03:22:48.905365Z","shell.execute_reply":"2024-07-30T03:28:40.145393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!activate clip_prefix_caption","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:35:00.772020Z","iopub.execute_input":"2024-07-30T03:35:00.772846Z","iopub.status.idle":"2024-07-30T03:35:02.015249Z","shell.execute_reply.started":"2024-07-30T03:35:00.772812Z","shell.execute_reply":"2024-07-30T03:35:02.013818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:37:59.903560Z","iopub.execute_input":"2024-07-30T03:37:59.904269Z","iopub.status.idle":"2024-07-30T03:38:13.030436Z","shell.execute_reply.started":"2024-07-30T03:37:59.904234Z","shell.execute_reply":"2024-07-30T03:38:13.029316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda install -y gdown","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:44:26.672272Z","iopub.execute_input":"2024-07-30T03:44:26.672664Z","iopub.status.idle":"2024-07-30T03:45:49.200047Z","shell.execute_reply.started":"2024-07-30T03:44:26.672633Z","shell.execute_reply":"2024-07-30T03:45:49.199064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install openai-clip","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:46:41.814609Z","iopub.execute_input":"2024-07-30T03:46:41.815242Z","iopub.status.idle":"2024-07-30T03:46:56.775239Z","shell.execute_reply.started":"2024-07-30T03:46:41.815210Z","shell.execute_reply":"2024-07-30T03:46:56.774136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Imports\n\nimport clip\nimport os\nfrom torch import nn\nimport numpy as np\nimport torch\nimport torch.nn.functional as nnf\nimport sys\nfrom typing import Tuple, List, Union, Optional\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, trange\nimport skimage.io as io\nimport PIL.Image\nfrom IPython.display import Image \nfrom enum import Enum\n\n\n\nN = type(None)\nV = np.array\nARRAY = np.ndarray\nARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\nVS = Union[Tuple[V, ...], List[V]]\nVN = Union[V, N]\nVNS = Union[VS, N]\nT = torch.Tensor\nTS = Union[Tuple[T, ...], List[T]]\nTN = Optional[T]\nTNS = Union[Tuple[TN, ...], List[TN]]\nTSN = Optional[TS]\nTA = Union[T, ARRAY]\n\n\nD = torch.device\nCPU = torch.device('cpu')\n\n\ndef get_device(device_id: int) -> D:\n    if not torch.cuda.is_available():\n        return CPU\n    device_id = min(torch.cuda.device_count() - 1, device_id)\n    return torch.device(f'cuda:{device_id}')\n\n\nCUDA = get_device\n\ncurrent_directory = os.getcwd()\nsave_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\nos.makedirs(save_path, exist_ok=True)\nmodel_path = os.path.join(save_path, 'model_wieghts.pt')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:49:52.708274Z","iopub.execute_input":"2024-07-30T03:49:52.708963Z","iopub.status.idle":"2024-07-30T03:49:52.720314Z","shell.execute_reply.started":"2024-07-30T03:49:52.708929Z","shell.execute_reply":"2024-07-30T03:49:52.719448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Model\n\n\nclass MappingType(Enum):\n    MLP = 'mlp'\n    Transformer = 'transformer'\n\n\nclass MlpTransformer(nn.Module):\n     def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n         super().__init__()\n         out_d = out_d if out_d is not None else in_dim\n         self.fc1 = nn.Linear(in_dim, h_dim)\n         self.act = act\n         self.fc2 = nn.Linear(h_dim, out_d)\n         self.dropout = nn.Dropout(dropout)\n\n     def forward(self, x):\n         x = self.fc1(x)\n         x = self.act(x)\n         x = self.dropout(x)\n         x = self.fc2(x)\n         x = self.dropout(x)\n         return x\n\nclass MLP(nn.Module):\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)\n\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) - 1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim_self // num_heads\n        self.scale = head_dim ** -0.5\n        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n        self.project = nn.Linear(dim_self, dim_self)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, y=None, mask=None):\n        y = y if y is not None else x\n        b, n, c = x.shape\n        _, m, d = y.shape\n        # b n h dh\n        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n        # b m 2 h dh\n        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n        if mask is not None:\n            if mask.dim() == 2:\n                mask = mask.unsqueeze(1)\n            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n        attention = attention.softmax(dim=2)\n        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n        out = self.project(out)\n        return out, attention\n\n\nclass TransformerLayer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        x_, attention = self.attn(self.norm1(x), y, mask)\n        x = x + x_\n        x = x + self.mlp(self.norm2(x))\n        return x, attention\n\n    def forward(self, x, y=None, mask=None):\n        x = x + self.attn(self.norm1(x), y, mask)[0]\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n                 norm_layer: nn.Module = nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim_self)\n        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n        self.norm2 = norm_layer(dim_self)\n        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n\n\nclass Transformer(nn.Module):\n\n    def forward_with_attention(self, x, y=None, mask=None):\n        attentions = []\n        for layer in self.layers:\n            x, att = layer.forward_with_attention(x, y, mask)\n            attentions.append(att)\n        return x, attentions\n\n    def forward(self, x, y=None, mask=None):\n        for i, layer in enumerate(self.layers):\n            if i % 2 == 0 and self.enc_dec: # cross\n                x = layer(x, y)\n            elif self.enc_dec:  # self\n                x = layer(x, x, mask)\n            else:  # self or cross\n                x = layer(x, y, mask)\n        return x\n\n    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n        super(Transformer, self).__init__()\n        dim_ref = dim_ref if dim_ref is not None else dim_self\n        self.enc_dec = enc_dec\n        if enc_dec:\n            num_layers = num_layers * 2\n        layers = []\n        for i in range(num_layers):\n            if i % 2 == 0 and enc_dec:  # cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            elif enc_dec:  # self\n                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n            else:  # self or cross\n                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n        self.layers = nn.ModuleList(layers)\n\n\nclass TransformerMapper(nn.Module):\n\n    def forward(self, x):\n        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n        prefix = torch.cat((x, prefix), dim=1)\n        out = self.transformer(prefix)[:, self.clip_length:]\n        return out\n\n    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n        super(TransformerMapper, self).__init__()\n        self.clip_length = clip_length\n        self.transformer = Transformer(dim_embedding, 8, num_layers)\n        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)\n\n\nclass ClipCaptionModel(nn.Module):\n\n    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n\n    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n                labels: Optional[torch.Tensor] = None):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n        return out\n\n    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        if mapping_type == MappingType.MLP:\n            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n                                     self.gpt_embedding_size * prefix_length))\n        else:\n            self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n                                                                     clip_length, num_layers)\n\n\nclass ClipCaptionPrefix(ClipCaptionModel):\n\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:50:17.430383Z","iopub.execute_input":"2024-07-30T03:50:17.430740Z","iopub.status.idle":"2024-07-30T03:50:17.474690Z","shell.execute_reply.started":"2024-07-30T03:50:17.430710Z","shell.execute_reply":"2024-07-30T03:50:17.473697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Caption prediction\n\ndef generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n                  entry_length=67, temperature=1., stop_token: str = '.'):\n\n    model.eval()\n    stop_token_index = tokenizer.encode(stop_token)[0]\n    tokens = None\n    scores = None\n    device = next(model.parameters()).device\n    seq_lengths = torch.ones(beam_size, device=device)\n    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n    with torch.no_grad():\n        if embed is not None:\n            generated = embed\n        else:\n            if tokens is None:\n                tokens = torch.tensor(tokenizer.encode(prompt))\n                tokens = tokens.unsqueeze(0).to(device)\n                generated = model.gpt.transformer.wte(tokens)\n        for i in range(entry_length):\n            outputs = model.gpt(inputs_embeds=generated)\n            logits = outputs.logits\n            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n            logits = logits.softmax(-1).log()\n            if scores is None:\n                scores, next_tokens = logits.topk(beam_size, -1)\n                generated = generated.expand(beam_size, *generated.shape[1:])\n                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n                if tokens is None:\n                    tokens = next_tokens\n                else:\n                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n                    tokens = torch.cat((tokens, next_tokens), dim=1)\n            else:\n                logits[is_stopped] = -float(np.inf)\n                logits[is_stopped, 0] = 0\n                scores_sum = scores[:, None] + logits\n                seq_lengths[~is_stopped] += 1\n                scores_sum_average = scores_sum / seq_lengths[:, None]\n                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n                next_tokens_source = next_tokens // scores_sum.shape[1]\n                seq_lengths = seq_lengths[next_tokens_source]\n                next_tokens = next_tokens % scores_sum.shape[1]\n                next_tokens = next_tokens.unsqueeze(1)\n                tokens = tokens[next_tokens_source]\n                tokens = torch.cat((tokens, next_tokens), dim=1)\n                generated = generated[next_tokens_source]\n                scores = scores_sum_average * seq_lengths\n                is_stopped = is_stopped[next_tokens_source]\n            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n            generated = torch.cat((generated, next_token_embed), dim=1)\n            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n            if is_stopped.all():\n                break\n    scores = scores / seq_lengths\n    output_list = tokens.cpu().numpy()\n    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n    order = scores.argsort(descending=True)\n    output_texts = [output_texts[i] for i in order]\n    return output_texts\n\n\ndef generate2(\n        model,\n        tokenizer,\n        tokens=None,\n        prompt=None,\n        embed=None,\n        entry_count=1,\n        entry_length=67,  # maximum number of words\n        top_p=0.8,\n        temperature=1.,\n        stop_token: str = '.',\n):\n    model.eval()\n    generated_num = 0\n    generated_list = []\n    stop_token_index = tokenizer.encode(stop_token)[0]\n    filter_value = -float(\"Inf\")\n    device = next(model.parameters()).device\n\n    with torch.no_grad():\n\n        for entry_idx in trange(entry_count):\n            if embed is not None:\n                generated = embed\n            else:\n                if tokens is None:\n                    tokens = torch.tensor(tokenizer.encode(prompt))\n                    tokens = tokens.unsqueeze(0).to(device)\n\n                generated = model.gpt.transformer.wte(tokens)\n\n            for i in range(entry_length):\n\n                outputs = model.gpt(inputs_embeds=generated)\n                logits = outputs.logits\n                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n                                                    ..., :-1\n                                                    ].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n                logits[:, indices_to_remove] = filter_value\n                next_token = torch.argmax(logits, -1).unsqueeze(0)\n                next_token_embed = model.gpt.transformer.wte(next_token)\n                if tokens is None:\n                    tokens = next_token\n                else:\n                    tokens = torch.cat((tokens, next_token), dim=1)\n                generated = torch.cat((generated, next_token_embed), dim=1)\n                if stop_token_index == next_token.item():\n                    break\n\n            output_list = list(tokens.squeeze().cpu().numpy())\n            output_text = tokenizer.decode(output_list)\n            generated_list.append(output_text)\n\n    return generated_list[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:50:34.018661Z","iopub.execute_input":"2024-07-30T03:50:34.019127Z","iopub.status.idle":"2024-07-30T03:50:34.052637Z","shell.execute_reply.started":"2024-07-30T03:50:34.019085Z","shell.execute_reply":"2024-07-30T03:50:34.051578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gdown \npretrained_model = 'COCO'  \nurl = 'https://drive.google.com/uc?id=1GYPToCqFREwi285wPLhuVExlz7DDUDfJ' \ngdown.download(url, model_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:51:51.519036Z","iopub.execute_input":"2024-07-30T03:51:51.519675Z","iopub.status.idle":"2024-07-30T03:52:05.054542Z","shell.execute_reply.started":"2024-07-30T03:51:51.519643Z","shell.execute_reply":"2024-07-30T03:52:05.053673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"is_gpu = True #@param {type:\"boolean\"}  ","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:53:15.300708Z","iopub.execute_input":"2024-07-30T03:53:15.301342Z","iopub.status.idle":"2024-07-30T03:53:15.305875Z","shell.execute_reply.started":"2024-07-30T03:53:15.301308Z","shell.execute_reply":"2024-07-30T03:53:15.304807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title CLIP model + GPT2 tokenizer\n\ndevice = CUDA(0) if is_gpu else \"cpu\"\nclip_model, preprocess = clip.load(\"RN50x4\", device=device, jit=False)\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:53:31.994909Z","iopub.execute_input":"2024-07-30T03:53:31.995280Z","iopub.status.idle":"2024-07-30T03:53:43.590537Z","shell.execute_reply.started":"2024-07-30T03:53:31.995253Z","shell.execute_reply":"2024-07-30T03:53:43.589409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-07-30T04:12:47.444090Z","iopub.execute_input":"2024-07-30T04:12:47.445155Z","iopub.status.idle":"2024-07-30T04:12:59.663174Z","shell.execute_reply.started":"2024-07-30T04:12:47.445115Z","shell.execute_reply":"2024-07-30T04:12:59.662014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Load model weights\n\nprefix_length = 40\n\nmodel = ClipCaptionPrefix(prefix_length, clip_length=40, prefix_size=640,\n                                  num_layers=8, mapping_type='transformer')\n# Load state dictionary with strict=False to ignore unexpected keys\nmodel.load_state_dict(torch.load(model_path, map_location=CPU), strict=False) \n\nmodel = model.eval() \ndevice = CUDA(0) if is_gpu else \"cpu\"\nmodel = model.to(device)\n\n# Print a summary of the loaded model to inspect its structure\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T04:13:09.110313Z","iopub.execute_input":"2024-07-30T04:13:09.110699Z","iopub.status.idle":"2024-07-30T04:13:11.064747Z","shell.execute_reply.started":"2024-07-30T04:13:09.110667Z","shell.execute_reply":"2024-07-30T04:13:11.063817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom skimage import io\nimport PIL\nfrom PIL import Image\nfrom torchvision import transforms\nimport json\n\nimage_dir = \"/kaggle/input/coco-2017-dataset/coco2017/val2017\"\ncaptions_data = []\n\nfor filename in os.listdir(image_dir):\n    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Add other image extensions if needed\n        file_path = os.path.join(image_dir, filename)\n        \n        # Extract image ID from filename (assuming filename format is like \"000000000123.jpg\")\n        image_id = os.path.splitext(filename)[0]\n        \n        image = io.imread(file_path)\n        pil_image = PIL.Image.fromarray(image)\n        display(pil_image)\n        \n        image = preprocess(pil_image).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n            prefix = prefix / prefix.norm(2, -1).item()\n            prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n        \n            generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n        \n        # Store both image ID and caption\n        captions_data.append({\n            \"id\": image_id,\n            \"caption\": generated_text_prefix\n        })\n\noutput_file = \"captions1.json\"\nwith open(output_file, \"w\") as f:\n    json.dump(captions_data, f, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}